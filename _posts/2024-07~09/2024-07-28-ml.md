---
layout: post
title:  "机器学习笔记"
date:  2024-07-28
tags: [AI]
---

  再次看机器学习相关内容，之前有些疑惑的点，有更深的体会了，特此记录。

# 小结

* 假设函数的定义比我想的要重要，是用于的拟合的基础假设，直接决定模型的好坏
  * 比如：`y = a1 + a2*x1` 和 `y = a1 + a2 * x1 + a3 * x2^2` 的拟合能力差异
  * 多层神经网络，便是对模型的假设定义，用尽可能的高维空间来做后续目标测试集的拟合
* 损失函数，取自假设模型，其过程是将假设函数尽可能地拟合到目标测试集
  * 所以，所谓 garbage in garbage out，用于拟合的数据或训练集都不是最优的需要拟合的模型，那么最终的拟合出来的模型，势必是无法使用的。
* 线性回归本身已包含很多人工智能的思路和算法，比如模型假设、内部参数、损失函数、使用梯度下降最小化损失
  * ![WX20240728-204032@2x](https://github.com/user-attachments/assets/747633c9-4139-4d95-8085-b54fae7421da)

* 数学计算上的梯度下降，计算的是整体数据集的导数方向，但由于大规模数据，一般的 batch 就选择子集来近似获取相关向量






# octave

  参考：https://octave.org/

```sh
$ brew install octave
$ octave --gui
```

# 参考

* [吴恩达机器学习系列课程](https://www.bilibili.com/video/BV164411b7dx?p=11)
  * 目前看到 11 集，下次 12 集开始。
