---
layout: post
title:  "面向开发工程师的 0 基础机器学习教程 - 激活函数（二）"
date:  2025-09-17
tags: [python, AI]
use_math: true
---

面向开发工程师的 0 基础机器学习教程 - 激活函数（二）

接上次 `y = ax + b` [最简单的机器学习算法](https://ata.atatech.org/articles/11020476406)之后，本次从二维升级到三维。以一个最简单的「与」门开关为例，解释为什么需要激活函数，什么是线形，什么是非线形的引入等。

训练一个新的机器学习模型，测试数据为：

* 输入 `x1 = 0, x2 = 0`，则输出 `y = 0`
* 输入 `x1 = 0, x2 = 1`，则输出 `y = 0`
* 输入 `x1 = 1, x2 = 0`，则输出 `y = 0`
* 输入 `x1 = 1, x2 = 1`，则输出 `y = 1`

可以看到之前仅涉及输入输出两个变量，一个二维空间就足够，这次数据涉及三个变量，所以本地的模型需要构建一个三维空间来做相关预测。

随着上次概念的展开，`a` 是权重，`b` 是偏置，本次 `a` 用 `w` 来代替。所以，本次的预测模型为：

$$
y = w_1 * x_1 + w_2 * x_2 + b
$$

# 实现上述机器学习

和之前 `y = ax + b` 类似，只是多了一个 `w2` 参数，所以，相对比较简单，依样画葫芦就行，其更新算法如下：

$$
\begin{aligned}
y &= w_1 * x_1 + w_2 * x_2 + b; \text{\hspace{1em}预测模型定义}\\
L &= (y - \hat{y}) ^ 2 \text{\hspace{7em}自定义的损失函数}\\
\frac{\partial L}{\partial w_2} &= \frac{\partial L}{\partial y} · \frac{\partial y}{\partial w_2} \text{\hspace{7em}w2更新偏导}\\
&= 2 * (y - \hat{y}) * x2
\end{aligned}
$$

```py
# 学习率
rate = 0.1

# 「与门」的测试数据
x = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [0, 0, 0, 1]

# 权重和偏置
w1 = 1.0;
w2 = 1.0;
b = 1.0;

# 预测模型
def predict(x1, x2):
    return w1 * x1 + w2 * x2 + b;

# 模型训练迭代
epoch = 1000
while epoch > 0:
    epoch = epoch - 1;
    
    deltaW1 = 0
    deltaW2 = 0
    deltaB = 0
    # 遍历测试数据集
    for xs, yhat in zip(x, y):
        [x1, x2] = xs
        deltaW1 += 2 * (predict(x1, x2) - yhat) * x1
        # 多加了这段代码，其他代码都类似
        deltaW2 += 2 * (predict(x1, x2) - yhat) * x2
        deltaB += 2 * (predict(x1, x2) - yhat) * 1
    deltaW1 = deltaW1 / len(x)
    deltaW2 = deltaW2 / len(x)
    deltaB = deltaB / len(x)

    # 更新
    w1 = w1 - rate * deltaW1
    w2 = w2 - rate * deltaW2
    b = b - rate * deltaB

print("y = " + str(w1) + "x1 + " + str(w2) + "x2 + " + str(b))
# 输出
# y = 0.4999999999999979x1 + 0.4999999999999979x2 + -0.24999999999999753
# y = 0.5x1 + 0.5x2 - 0.25
```

相关训练完成的模型为：`y = 0.5 * x1 + 0.5 * x2 - 0.25`，其在三维空间的平台大致如下：

训练后的模型实际预测值如下：
* 输入 `x1 = 0, x2 = 0`，则输出 `y = -0.25`
* 输入 `x1 = 0, x2 = 1`，则输出 `y = 0.25`
* 输入 `x1 = 1, x2 = 0`，则输出 `y = 0.25`
* 输入 `x1 = 1, x2 = 1`，则输出 `y = 0.75`

> 红色点为 [0, 0, 0]， 蓝色点为 [0, 1, 0] 和 [1, 0, 0], 绿色点为 [1, 1, 1]
>
> 可以看到训练完成的模型线性平面，已经尽可能接近这三个点，但输入输出的误差依旧比较大，差值都在 0.25，距离相等。

![](https://img.alicdn.com/imgextra/i3/O1CN01fJMcJ529XmVhtToZ5_!!6000000008078-2-tps-1280-960.png)

这便是预测模型定义的限制，因为模型定义是 `y = w1 * x1 + w2 * x2 + b`，从定义出来，整个模型就是线形的，就仅仅是一个平面而已，所以，这便是这个预测模型的上限（即误差不可能再低了）。


# 激活函数 ReLU

为了解决上述误差较大的问题，或者说预测模型不具备非线形的问题，需引入一个非线形函数，即增加一个激活函数，本次先选用 ReLU 激活函数，来看看激活函数的增加，会对整体的模型预测有什么影响。

ReLU 函数非常简单，就是 x 小于 0，就取 0，x 大于 0，就输出 x 原样不变，大致如下：（PS：ReLU 的求导也就比较简单了，要不就是 0，要不就是 1）

![](https://img.alicdn.com/imgextra/i1/O1CN01mPGNCU276BPfEiRjA_!!6000000007747-2-tps-2880-1632.png)

激活函数，就是对原有的函数值，做一次非线形变换，使得模型整体具备非线性特性，新的模型定义如下：

$$
\begin{aligned}
y &= ReLU(w_1 * x_1 + w_2 * x_2 + b); \text{\hspace{1em}预测模型定义}\\
\text{ReLU}(x) &=
\begin{cases}
x, & \text{if } x > 0 \\
0, & \text{otherwise}
\end{cases} \text{\hspace{7em}非线形的激活函数}\\
\end{aligned}
$$

由新模型的定义出发，可以得到如下权重的更新算法，为方便起见，新增一个 `h` 函数，替换原有的函数：

$$
\begin{aligned}
L &= (y - \hat{y}) ^ 2 \text{\hspace{8em}自定义的损失函数}\\
y &= ReLU(h) \text{\hspace{7em}预测模型定义}\\
h &= w_1 * x_1 + w_2 * x_2 + b \text{\hspace{2em}原函数的定义}\\
\frac{\partial L}{\partial w_2} &= \frac{\partial L}{\partial y} · \frac{\partial y}{\partial h} · \frac{\partial h}{\partial w_2} \text{\hspace{5em}链式法则 w2 更新偏导数}\\
&= 
\begin{cases}
2 * (y - \hat{y}) * x2, & \text{if } h > 0 \\
0, & \text{otherwise}
\end{cases}
\end{aligned}
$$

偏置 `b` 的更新类似，所以有如下代码：

```py
rate = 0.1

# 与门测试数据
x = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [0, 0, 0, 1]

w1 = 1.0;
w2 = 1.0;
b = 1.0;

# 新增的 relu 激活函数
def relu(v):
    if v < 0:
        return 0
    else:
        return v

# 原函数的定义
def h(x1, x2):
    return w1 * x1 + w2 * x2 + b

# 新的预测模型函数定义
def predict(x1, x2):
    return relu(h(x1, x2));

# 训练迭代
epoch = 10000
while epoch > 0:
    epoch = epoch - 1;
    
    # w 和 b 更新算法
    deltaW1 = 0
    deltaW2 = 0
    deltaB = 0

    # 遍历测试数据集
    for xs, yhat in zip(x, y):
        [x1, x2] = xs
        # 如果原函数的值，大于 0，才更新到误差中，否则就是 0，不更新        
        if h(x1, x2) > 0:
            deltaW1 += 2 * (predict(x1, x2) - yhat) * x1
            deltaW2 += 2 * (predict(x1, x2) - yhat) * x2
            deltaB += 2 * (predict(x1, x2) - yhat) * 1
    deltaW1 = deltaW1 / len(x)
    deltaW2 = deltaW2 / len(x)
    deltaB = deltaB / len(x)

    # 更新
    w1 = w1 - rate * deltaW1
    w2 = w2 - rate * deltaW2
    b = b - rate * deltaB

print("y = ReLU(" + str(w1) + "x1 + " + str(w2) + "x2 + " + str(b) + ")")
```

训练完成后，可以得到预测模型如下：`y = ReLU(0.9999999999999944x1 + 0.9999999999999944x2 + -0.9999999999999922)`，即 `y = ReLU(x1 + x2 - 1)`

将上述平台可视化后，可以得到如下平台：

![](https://img.alicdn.com/imgextra/i2/O1CN01Tsf3Wa1VCdayPnh24_!!6000000002617-2-tps-1280-960.png)

可以看到，添加了 ReLU 激活函数后，整体的模型损失趋近于 0，效果非常不错，这就是非线形激活函数给原来线形函数所带来的更强的拟合效果。


# Sigmoid 激活函数

除了 ReLU，还有一个常用的激活函数是 Sigmod，其函数定义如下：

$$
\text{Sigmoid}(x) = S(x) = \frac{1}{1 + e^{-x}}
$$

大概长这样

![](https://img.alicdn.com/imgextra/i1/O1CN01Opubju1LWekVogN21_!!6000000001307-2-tps-400-400.png)

其导数为[1]：

$$
S'(x) = S(x)(1 - S(x))
$$

预测模型及其更新算法为：
$$
\begin{aligned}
L &= (y - \hat{y}) ^ 2 \text{\hspace{8em}自定义的损失函数}\\
y &= Sigmoid(h) \text{\hspace{7em}预测模型定义}\\
h &= w_1 * x_1 + w_2 * x_2 + b \text{\hspace{2em}原函数的定义}\\
\frac{\partial L}{\partial w_2} &= \frac{\partial L}{\partial y} · \frac{\partial y}{\partial h} · \frac{\partial h}{\partial w_2} \text{\hspace{5em}w2更新偏导}\\
&= 2 * (y - \hat{y}) * (sigmoid 对 h 求导) * x2 \text{\hspace{2em}代入求导结果} \\
&= 2 * (y - \hat{y}) * sigmoid(h) * (1 - sigmoid(h)) * x2
\end{aligned}
$$


所以代码实现如下：

```py
import math
rate = 0.1

# and 测试数据
x = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [0, 0, 0, 1]

w1 = 1.0;
w2 = 1.0;
b = 1.0;

# sigmoid 激活函数
def sigmoid(x):
  return 1 / (1 + math.exp(-x))

# sigmoid 求导
def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

# 原函数
def h(x1, x2):
    return w1 * x1 + w2 * x2 + b

# 预测模型
def predict(x1, x2):
    return sigmoid(h(x1, x2))

# 模型迭代
epoch = 10000
while epoch > 0:
    epoch = epoch - 1;
    
    deltaW1 = 0
    deltaW2 = 0
    deltaB = 0

    # 遍历测试集
    for xs, yhat in zip(x, y):
        [x1, x2] = xs
        # 添加额外的 sigmoid 导数
        deltaW1 += 2 * (predict(x1, x2) - yhat) * sigmoid_derivative(h(x1, x2))  * x1
        deltaW2 += 2 * (predict(x1, x2) - yhat) * sigmoid_derivative(h(x1, x2)) * x2
        deltaB += 2 * (predict(x1, x2) - yhat) * sigmoid_derivative(h(x1, x2)) * 1
            
    deltaW1 = deltaW1 / len(x)
    deltaW2 = deltaW2 / len(x)
    deltaB = deltaB / len(x)

    # 更新
    w1 = w1 - rate * deltaW1
    w2 = w2 - rate * deltaW2
    b = b - rate * deltaB

print("y = Sigmoid(" + str(w1) + "x1 + " + str(w2) + "x2 + " + str(b) + ")")
```

训练完成后的模型为：`y = Sigmoid(4.649822106307222x1 + 4.649822106307222x2 + -7.0720835183397455)` 即 `y = sigmoid(4.65x1 + 4.65x2 - 7)`

其可视化后的曲面如下：

![](https://img.alicdn.com/imgextra/i1/O1CN01elr2UV1zS9U7CMhJe_!!6000000006712-2-tps-1280-960.png)


# Pytorch 版本

## ReLU

```py
import torch
import torch.nn as nn

# 测试数据
x = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
y = torch.tensor([[0.0], [0.0], [0.0], [1.0]])

# 创建预测模型，包含线性层，2个输入、一个输出，并加一个 ReLU 激活函数
model = nn.Sequential(nn.Linear(2, 1), nn.ReLU())

# 定义均方误差损失函数
mseloss = torch.nn.MSELoss()
# 随机梯度下降优化器
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # 学习率为0.1， learning rate

# 执行梯度下降算法进行模型训练
for epoch in range(10000):
    y_pred = model(x)  # 计算预测值
    loss = mseloss(y_pred, y)  # 计算损失
    
    optimizer.zero_grad()  # 清零梯度
    loss.backward()  # 反向传播，计算梯度
    optimizer.step()  # 更新模型参数

# 打印模型
for layer in model.children():
    if isinstance(layer, nn.Linear):
        print("权重 w " + str(layer.state_dict()['weight']))
        print("偏置 b " + str(layer.state_dict()['bias']))
```

输出（符合预期）

```
权重 w tensor([[1.0000, 1.0000]])
偏置 b tensor([-1.0000])
```

## Sigmoid

```py
import torch
import torch.nn as nn

# 测试数据
x = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
y = torch.tensor([[0.0], [0.0], [0.0], [1.0]])

# 创建预测模型，包含线性层，2个输入、一个输出，并加一个 ReLU 激活函数
model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())

# 定义均方误差损失函数
mseloss = torch.nn.MSELoss()
# 随机梯度下降优化器
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # 学习率为0.1， learning rate

# 执行梯度下降算法进行模型训练
for epoch in range(10000):
    y_pred = model(x)  # 计算预测值
    loss = mseloss(y_pred, y)  # 计算损失
    
    optimizer.zero_grad()  # 清零梯度
    loss.backward()  # 反向传播，计算梯度
    optimizer.step()  # 更新模型参数

# 打印模型
for layer in model.children():
    if isinstance(layer, nn.Linear):
        print("权重 w " + str(layer.state_dict()['weight']))
        print("偏置 b " + str(layer.state_dict()['bias']))

```

输出（符合预期）

```
权重 w tensor([[4.6504, 4.6504]])
偏置 b tensor([-7.0730])
```


# 小结

可以看到在一个三维空间里，`y = w1 * x1 + w2 * x2 + b` 由于其线形特性，无法对 「与门」做较好拟合，在引入非线形的激活函数后，如：`y = ReLU(w1 * x1 + w2 * x2 + b)` 或 `y = Sigmoid(w1 * x1 + w2 * x2 + b)`，其 loss 损失值有较大的下降，取得了不错的效果。


# 相关系列

* [面向开发工程师的 0 基础机器学习教程 - 反向传播（一）最简单的机器学习算法，二维空间](https://zhoukekestar.github.io/notes/2025/09/17/ml.html)
* [面向开发工程师的 0 基础机器学习教程 - 激活函数（二）最简单的机器学习算法，三维空间](https://zhoukekestar.github.io/notes/2025/09/17/ml2.html)
* [面向开发工程师的 0 基础机器学习教程 - 神经网络（三）最简单的机器学习算法，神经网络](https://zhoukekestar.github.io/notes/2025/09/17/ml3.html)

限于本人学识与能力，文中难免存在疏漏与不足之处，恳请读者不吝指正。


# 参考

1. [Sigmoid函数求导](https://zhuanlan.zhihu.com/p/452684332)
2. [通用近似定理/万能近似定理](https://zh.wikipedia.org/wiki/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86)

