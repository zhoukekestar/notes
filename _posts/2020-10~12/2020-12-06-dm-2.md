---
layout: post
title:  "数据挖掘（第二章）"
date:  2020-12-06
tags: [data-mining]
---

学堂在线，[《数据挖掘：理论与算法》](https://www.xuetangx.com/course/THU08091000385/4233665?channel=search_result) 学习简单记录。


# 第二章 数据预处理



Outline

- 数据清洗
- 数据转换
- 数据描述
- 特征选择
- 特征提取



## 为什么？
数据预处理是 DM 中最大的挑战，也是最花时间的。
问题有：

- Incomplete，空值
- Noisy，填错了
- Inconsistent，矛盾和冲突
- Redundant，数据太多了或太少



Missing Data：

- 缺失情况：
   - Not Application，不适用的，比如向学生填写薪水
- 如何自动处理
   - 推测，固定值等等，是 Art



## 异常值和重复数据
异常点：最基础的思路，是检测每个点和近邻间的相对距离值。

- LOF 越大说明更容易是离群点。

重复值：通过排序（这里的设计思路要根据场景的不同来设计） + 滚动窗口的思路来处理大数据中的重复问题。

- 英文国家中，名很多都是 Bob，Tom 之类的，姓确很复杂。



## 数据转换和采样


数据类型中 Nominal 中的编码问题，在计算机中类似是枚举问题，很可能会因为编码的顺序问题，导致该属性的暗含距离问题（添加了编码的距离度量）。
较好的编码方式，可能是通过二进制表示，比如：0x01、0x02、0x04、0x08、0x10 ...


采样：
为什么需要？统计学中的采样是数据太多，大数据中的采样，是数据太多，机器资源太少导致的。
通过数据聚合来降低数据量，比如按日、月先聚合好数据


不平衡数据：
对于体验报告中的患病监测，不能单纯地按准确率来衡量一个分类器的好坏。
需要根据实际情况来判断，比如患病监测中，应尽可能找出患病。


解决不平衡：

- G-mean：正确和错误的分类，都需要分对。
- F-measure：搜索的准确率



Over-sampling
当数据量不平衡事，对少数据集，根据近邻自动生成一些点。 SMOTE


Boundary Sampling
大数据量下，仅有边缘点有效果

- 根据法向量等





## 数据描述和可视化


Normalization

- min-max normalization, 区间映射为 [0, 1]
- z-score normaliization，对于高斯分布，计算偏离值

![image](https://github.com/zhoukekestar/notes/assets/7157346/d281c87e-11bd-4d13-b254-6a9f4d50469f)


数据描述

- 平均值
- 中位数
- 众数(mode)
- 方差（variance）, 离散程度
- 数据相关性
   - r(a,b) > 0 正相关
   - r(a,b) < 0 负相关
   - r(a,b) = 0 无线性相关，而不是不相关
- chi-square
   - 和预期的平方和





高维可视化

- Box Plots
   - 上下沿是 25%，75%
   - 上下的红点为离散点
   - 丧失了多维之间的关系
- Parallel Coordinates，平行坐标
   - 每条线是一个高维的点



软件

- citespace，文献引用关系
- Gephi，关系图、社交网络等





## 特征选择


特性核心用来区分不同 group，能区分出不同的 group 的特征值就是好的特征。


Entropy （熵）用来衡量不确定性

- 0 或 1 都是高度的不确定
- 0.5 为相对确定



H(S|X) 为某个特性下的熵，Gain(S,X) = H(S) - H(S | X) 为信息增益 information gain


Feature Subset Search，特性子集搜索，用来选取更少的特征性，来获取更多的信息。

- Exhaustive 枚举
- Search and Bound 基于特征值越少，值会单调递减，做一些剪枝。
- Top K Feature，由于属性可能会包含，所以此方式不见得最优
- 向前、向后 search

相关算法：

- Simulated Annealing, 模拟退火
- Tabu Search
- Genetic Algorithms，遗传算法





## 特性提取


Principal Component Analysis（PCA）：主成分分析提取

- Lagrange Multipliers，拉格朗日乘数法



将原始数据投影到特征向量上，且特征值最大的向量上。


PCA 不考虑分类问题，分类问题，需要通过 LDA（Liner Discriminant Analysis） 做投影。
LDA 保留的是类的区分信息。


LDA

- Fisher 判断准则





